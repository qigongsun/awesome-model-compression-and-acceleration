#### low-bit quantization
- [BinaryConnect: Training Deep Neural Networks with binary weights during propagations](https://arxiv.org/pdf/1511.00363.pdf)
- [Local Binary Convolutional Neural Networks](https://arxiv.org/pdf/1608.06049.pdf)
- [Binarized Convolutional Neural Networks with Separable Filters for Efficient Hardware Acceleration](https://arxiv.org/pdf/1707.04693.pdf)
- [Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](https://arxiv.org/pdf/1602.02830.pdf)
- [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](https://arxiv.org/pdf/1603.05279.pdf)
- [DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients](https://arxiv.org/pdf/1606.06160.pdf)
- [Regularizing activation distribution for training binarized deep networks](http://openaccess.thecvf.com/content_CVPR_2019/papers/Ding_Regularizing_Activation_Distribution_for_Training_Binarized_Deep_Networks_CVPR_2019_paper.pdf)
- [Towards accurate binary convolutional neural network](https://arxiv.org/pdf/1711.11294.pdf)
- [Lq-nets: Learned quantization for highly accurate and compact deep neural networks](https://arxiv.org/pdf/1807.10029.pdf)
- [Two-step quantization for low-bit neural networks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Two-Step_Quantization_for_CVPR_2018_paper.pdf)
- [Alternating Multi-bit Quantization for Recurrent Neural Networks](https://arxiv.org/pdf/1802.00150.pdf)
- [PACT: Parameterized Clipping Activation for Quantized Neural Networks](https://arxiv.org/pdf/1805.06085.pdf)

#### high-bit quantization
- [Quantize weights and activations in Recurrent Neural Networks](https://arxiv.org/pdf/1611.10176.pdf)
- [The ZipML Framework for Training Models with End-to-End Low Precision: The Cans, the Cannots, and a Little Bit of Deep Learning](https://arxiv.org/pdf/1611.05402.pdf)
- [Quantized Convolutional Neural Networks for Mobile Devices](https://arxiv.org/pdf/1512.06473.pdf)
- [Compressing Deep Convolutional Networks using Vector Quantization](https://arxiv.org/pdf/1412.6115.pdf)
- [Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations](https://arxiv.org/pdf/1609.07061.pdf)
- [Fixed-Point Performance Analysis of Recurrent Neural Networks](https://arxiv.org/abs/1512.01322)
- [Loss-aware Binarization of Deep Networks](https://arxiv.org/pdf/1611.01600.pdf)
- [Towards the Limit of Network Quantization](https://arxiv.org/pdf/1612.01543.pdf)
- [Deep Learning with Low Precision by Half-wave Gaussian Quantization](https://arxiv.org/pdf/1702.00953.pdf)
- [ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks](https://arxiv.org/pdf/1706.02393.pdf)
- [Trained Ternary Quantization](https://arxiv.org/pdf/1612.01064.pdf)
- [Defensive Quantization: When Efficiency Meets Robustness](https://arxiv.org/pdf/1904.08444.pdf)
- [Differentiable soft quantization: Bridging full-precision and low-bit neural networks](http://openaccess.thecvf.com/content_ICCV_2019/papers/Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf)
- [HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision](https://arxiv.xilesou.top/pdf/1905.03696.pdf)
- [Structured Binary Neural Networks for Image Recognition](https://arxiv.xilesou.top/pdf/1909.09934.pdf)
- [Mixed Precision DNNs: All you need is a good parametrization](https://openreview.net/pdf?id=Hyx0slrFvH)
- [AutoQB: AutoML for Network Quantization and Binarization on Mobile Devices](https://arxiv.xilesou.top/pdf/1902.05690.pdf)
- [Training Quantized Network with Auxiliary Gradient Module](https://arxiv.org/pdf/1903.11236.pdf)
- [GDRQ: Group-based Distribution Reshaping for Quantization](https://arxiv.xilesou.top/pdf/1908.01477.pdf)
- [On the Universal Approximability and Complexity Bounds of Quantized ReLU Neural Networks](https://arxiv.xilesou.top/pdf/1802.03646.pdf)
- [AAAI2019][Multi-Precision Quantized Neural Networks via Encoding Decomposition of -1 and+ 1](https://arxiv.org/pdf/1905.13389.pdf)
- [Differentiable Soft Quantization:Bridging Full-Precision and Low-Bit Neural Networks](http://openaccess.thecvf.com/content_ICCV_2019/papers/Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf)
- [Relaxed quantization for discretized neural networks](https://arxiv.xilesou.top/pdf/1810.01875.pdf)
- [Learning to Quantize Deep Networks byOptimizing Quantization Intervals with Task Loss](http://openaccess.thecvf.com/content_CVPR_2019/papers/Jung_Learning_to_Quantize_Deep_Networks_by_Optimizing_Quantization_Intervals_With_CVPR_2019_paper.pdf)

#### mix-bit quantization
- [2017][Mixed Low-precision Deep Learning Inference using Dynamic Fixed Point](https://arxiv.xilesou.top/pdf/1701.08978.pdf)
- [2017][Stochastic Quantization for Learning Accurate Low-Bit Deep Neural Networks](https://link_springer.xilesou.top/article/10.1007/s11263-019-01168-2)
- [2018][Mixed precision quantization of convnets via differentiable neural architecture search](https://arxiv.xilesou.top/pdf/1812.00090.pdf)
- [ICLR2018][Mixed precision training](https://arxiv.xilesou.top/pdf/1710.03740.pdf%EF%BC%89ã€‚)
- [2018][Highly scalable deep learning training system with mixed precision: Training imagenet in four minutes](https://arxiv.xilesou.top/abs/1807.11205)

- [2018][Mixed precision training of convolutional neural networks using integer operations](https://arxiv.xilesou.top/abs/1802.00930)

- [2018][Mixed precision architecture based on computational memory for training deep neural networks](https://ieeexplore_ieee.xilesou.top/abstract/document/8351656/)

- [2018][Lower numerical precision deep learning inference and training](https://www.intel.ai/nervana/wp-content/uploads/sites/53/2018/05/Lower-Numerical-Precision-Deep-Learning-Inference-Training.pdf)

- [2018][Quantization and training of neural networks for efficient integer-arithmetic-only inference](http://openaccess.thecvf.com/content_cvpr_2018/html/Jacob_Quantization_and_Training_CVPR_2018_paper.html)

- [2018][Adaptive quantization for deep neural network](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16248)

- [2019][Adaptive Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point Numbers](https://arxiv.xilesou.top/abs/1911.00361)

- [2019][Learning to quantize deep networks by optimizing quantization intervals with task loss](http://openaccess.thecvf.com/content_CVPR_2019/html/Jung_Learning_to_Quantize_Deep_Networks_by_Optimizing_Quantization_Intervals_With_CVPR_2019_paper.html)

- [2019][Differentiable quantization of deep neural networksarXiv:1905.11452](https://arxiv.org/abs/1905.11452)

- [2019][HAQ: Hardware-Aware Automated Quantization with Mixed Precision](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_HAQ_Hardware-Aware_Automated_Quantization_With_Mixed_Precision_CVPR_2019_paper.pdf)
- [2019][HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision](https://arxiv.xilesou.top/pdf/1905.03696.pdf)
- [2020R][Learning Architectures for Binary Networks](https://arxiv.org/pdf/2002.06963.pdf)
- [2019R][Any-Precision Deep Neural Networks](https://arxiv.org/pdf/1911.07346.pdf)
- [2020R][AdaBits: Neural Network Quantization with Adaptive Bit-Widths](https://arxiv.org/pdf/1912.09666.pdf)
- [2020R][Switchable Precision Neural Networks](https://arxiv.org/pdf/2002.02815.pdf)

