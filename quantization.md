#### low-bit quantization
- [BinaryConnect: Training Deep Neural Networks with binary weights during propagations](https://arxiv.org/pdf/1511.00363.pdf)
- [Local Binary Convolutional Neural Networks](https://arxiv.org/pdf/1608.06049.pdf)
- [Binarized Convolutional Neural Networks with Separable Filters for Efficient Hardware Acceleration](https://arxiv.org/pdf/1707.04693.pdf)
- [Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](https://arxiv.org/pdf/1602.02830.pdf)
- [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](https://arxiv.org/pdf/1603.05279.pdf)
- [DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients](https://arxiv.org/pdf/1606.06160.pdf)
- [Regularizing activation distribution for training binarized deep networks](http://openaccess.thecvf.com/content_CVPR_2019/papers/Ding_Regularizing_Activation_Distribution_for_Training_Binarized_Deep_Networks_CVPR_2019_paper.pdf)
- [Towards accurate binary convolutional neural network](https://arxiv.org/pdf/1711.11294.pdf)
- [Lq-nets: Learned quantization for highly accurate and compact deep neural networks](https://arxiv.org/pdf/1807.10029.pdf)
- [Two-step quantization for low-bit neural networks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Two-Step_Quantization_for_CVPR_2018_paper.pdf)
- [Alternating Multi-bit Quantization for Recurrent Neural Networks](https://arxiv.org/pdf/1802.00150.pdf)
- [PACT: Parameterized Clipping Activation for Quantized Neural Networks](https://arxiv.org/pdf/1805.06085.pdf)

#### high-bit quantization
- [Quantize weights and activations in Recurrent Neural Networks](https://arxiv.org/pdf/1611.10176.pdf)
- [The ZipML Framework for Training Models with End-to-End Low Precision: The Cans, the Cannots, and a Little Bit of Deep Learning](https://arxiv.org/pdf/1611.05402.pdf)
- [Quantized Convolutional Neural Networks for Mobile Devices](https://arxiv.org/pdf/1512.06473.pdf)
- [Compressing Deep Convolutional Networks using Vector Quantization](https://arxiv.org/pdf/1412.6115.pdf)
- [Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations](https://arxiv.org/pdf/1609.07061.pdf)
- [Fixed-Point Performance Analysis of Recurrent Neural Networks](https://arxiv.org/abs/1512.01322)
- [Loss-aware Binarization of Deep Networks](https://arxiv.org/pdf/1611.01600.pdf)
- [Towards the Limit of Network Quantization](https://arxiv.org/pdf/1612.01543.pdf)
- [Deep Learning with Low Precision by Half-wave Gaussian Quantization](https://arxiv.org/pdf/1702.00953.pdf)
- [ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks](https://arxiv.org/pdf/1706.02393.pdf)
- [Trained Ternary Quantization](https://arxiv.org/pdf/1612.01064.pdf)
- [Defensive Quantization: When Efficiency Meets Robustness](https://arxiv.org/pdf/1904.08444.pdf)
- [Differentiable soft quantization: Bridging full-precision and low-bit neural networks](http://openaccess.thecvf.com/content_ICCV_2019/papers/Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf) ResNet-18 MobileNetV2 3/3 4/4 实验对比
- [Structured Binary Neural Networks for Image Recognition](https://arxiv.xilesou.top/pdf/1909.09934.pdf)
- [GDRQ: Group-based Distribution Reshaping for Quantization](https://arxiv.xilesou.top/pdf/1908.01477.pdf)
- [On the Universal Approximability and Complexity Bounds of Quantized ReLU Neural Networks](https://arxiv.xilesou.top/pdf/1802.03646.pdf)
- [AAAI2019][Multi-Precision Quantized Neural Networks via Encoding Decomposition of -1 and+ 1](https://arxiv.org/pdf/1905.13389.pdf)
- [Differentiable Soft Quantization:Bridging Full-Precision and Low-Bit Neural Networks](http://openaccess.thecvf.com/content_ICCV_2019/papers/Gong_Differentiable_Soft_Quantization_Bridging_Full-Precision_and_Low-Bit_Neural_Networks_ICCV_2019_paper.pdf)
- [Relaxed quantization for discretized neural networks](https://arxiv.xilesou.top/pdf/1810.01875.pdf)
- [Learning to Quantize Deep Networks byOptimizing Quantization Intervals with Task Loss](http://openaccess.thecvf.com/content_CVPR_2019/papers/Jung_Learning_to_Quantize_Deep_Networks_by_Optimizing_Quantization_Intervals_With_CVPR_2019_paper.pdf)

#### mix-bit quantization
- [2017][Mixed Low-precision Deep Learning Inference using Dynamic Fixed Point](https://arxiv.xilesou.top/pdf/1701.08978.pdf)
- [2017][Stochastic Quantization for Learning Accurate Low-Bit Deep Neural Networks](https://link_springer.xilesou.top/article/10.1007/s11263-019-01168-2)
- [2018][Highly scalable deep learning training system with mixed precision: Training imagenet in four minutes](https://arxiv.xilesou.top/abs/1807.11205)
- [2018][Mixed precision quantization of convnets via differentiable neural architecture search](https://arxiv.xilesou.top/pdf/1812.00090.pdf)
- [ICLR2018][Mixed precision training](https://arxiv.xilesou.top/pdf/1710.03740.pdf%EF%BC%89。)

- [2018][Mixed precision training of convolutional neural networks using integer operations](https://arxiv.xilesou.top/abs/1802.00930)

- [2018][Mixed precision architecture based on computational memory for training deep neural networks](https://ieeexplore_ieee.xilesou.top/abstract/document/8351656/)

- [2018][Lower numerical precision deep learning inference and training](https://www.intel.ai/nervana/wp-content/uploads/sites/53/2018/05/Lower-Numerical-Precision-Deep-Learning-Inference-Training.pdf)

- [2018][Quantization and training of neural networks for efficient integer-arithmetic-only inference](http://openaccess.thecvf.com/content_cvpr_2018/html/Jacob_Quantization_and_Training_CVPR_2018_paper.html)

- [2018 AAAI][Adaptive quantization for deep neural network](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPaper/16248)
提出了一种估计各层参数量化误差对整体模型预测精度。然后，我们提议基于此测量的寻优过程每层的最佳量化比特宽度。

- [2019][Adaptive Precision Training: Quantify Back Propagation in Neural Networks with Fixed-point Numbers](https://arxiv.xilesou.top/abs/1911.00361)

- [2019 CVPR][Learning to quantize deep networks by optimizing quantization intervals with task loss](http://openaccess.thecvf.com/content_CVPR_2019/html/Jung_Learning_to_Quantize_Deep_Networks_by_Optimizing_Quantization_Intervals_With_CVPR_2019_paper.html) 
学习量化区间，包含多种bit的对比实验结果 W/A 2、3、4、5

- [2019][HAQ: Hardware-Aware Automated Quantization with Mixed Precision](http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_HAQ_Hardware-Aware_Automated_Quantization_With_Mixed_Precision_CVPR_2019_paper.pdf)
强化学习搜索网络每层的量化精度，MobileNet-V1， MobileNet-V2， Resnet-50

- [2019][HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision](https://arxiv.org/pdf/1905.03696.pdf)
- [2020R][Learning Architectures for Binary Networks](https://arxiv.org/pdf/2002.06963.pdf)
- [2019R][Any-Precision Deep Neural Networks](https://arxiv.org/pdf/1911.07346.pdf) 训练一个模型，不需要精调就可以适应多种bit的量化
- [2020R][AdaBits: Neural Network Quantization with Adaptive Bit-Widths](https://arxiv.org/pdf/1912.09666.pdf)
在本文中，我们研究了一种新的方法来实现这一目标，即启用自适应比特宽度模型中的权重和激活。我们首先检查训练量化模型的好处和挑战自适应比特宽度，然后使用几种方法进行实验，包括直接自适应、渐进式训练以及联合训练。我们发现联合训练能够在自适应模型上产生可比的性能个别模型。我们还提出了一种新技术可切换限幅电平（S-CL），进一步改善最低比特宽度的量化模型。我们的提议在包括MobileNet V1/V2和ResNet50在内的一系列模型上应用的技术，我们证明了位宽度权重和激活是自适应的可执行的深层神经网络，提供了一个明显的机会来提高准确性和效率基于平台约束的即时自适应实际应用程序。

- [2020R][Switchable Precision Neural Networks](https://arxiv.org/pdf/2002.02815.pdf)
本文提出了一种灵活的量化策略，称为可切换精确神经网络（SP-net），用来训练一个能够在多个量化级别上工作的共享网络。在运行时，网络可以根据即时内存、延迟、功耗和准确性动态调整其精度要求。MobileNet ResNet-18 2bit 8bit 32bit实验
- [2020R ICLR][Mixed Precision DNNs: All you need is a good parametrization](https://arxiv.org/pdf/1905.11452.pdf)
提出用步长和动态范围对量化器进行参数化。 MobileNetV2 ResNet-18 4bit实验
- [2020R ICLR][AutoQ: Automated Kernel-Wise Neural Network Quantization](https://arxiv.org/pdf/1902.05690.pdf)
利用强化学习来训练Kernel-Wise的量化神经网络， Resnet-18 Resnet-50 MobileNetV2 4bit左右的实验

- [2020 CVPR][Training Quantized Neural Networks with a Full-precision Auxiliary Module](https://arxiv.org/pdf/1903.11236.pdf) 全精度辅助模块训练量化神经网络

- [2019][Towards Efficient Training for Neural Network Quantization](https://arxiv.org/pdf/1912.10207.pdf)
训练量化网络 ResNet-50 MobileNetV1 MobileNetV2 2/2 3/3 4/4 bit 实验对比
